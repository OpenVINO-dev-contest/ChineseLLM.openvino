import os
import openvino as ov
from openvino.runtime import serialize
from transformers import AutoTokenizer, AutoModel
import torch
from pathlib import Path
import argparse

onnx_model_path = Path('onnx_model')
ir_model_path = Path('ir_model')
if onnx_model_path.exists() == False:
    os.mkdir(onnx_model_path)
if ir_model_path.exists() == False:
    os.mkdir(ir_model_path)

onnx_model = Path('onnx_model') / "chatglm2.onnx"
ir_model = Path('ir_model') / "chatglm2.xml"

from typing import List, Tuple

parser = argparse.ArgumentParser(add_help=False)
parser.add_argument('-h',
                    '--help',
                    action='help',
                    help='Show this help message and exit.')
parser.add_argument('-m',
                    '--model_id',
                    default='THUDM/chatglm2-6b',
                    required=False,
                    type=str,
                    help='orignal model path')
parser.add_argument('-cw',
                    '--compress_weight',
                    default=False,
                    required=False,
                    type=bool,
                    help='Weights Compression')
args = parser.parse_args()

query = "æƒ³è¦å‡ºå›½ç•™å­¦ï¼Œåº”è¯¥æ€ä¹ˆåŠï¼Ÿ"
history = [(
    "ä½ å¥½",
    "ä½ å¥½ğŸ‘‹!æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹, å¾ˆé«˜å…´è§åˆ°ä½ ,æ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚",
)]


def build_inputs(device,
                 tokenizer,
                 query: str,
                 history: List[Tuple[str, str]] = None):
    prompt = ""
    for i, (old_query, response) in enumerate(history):
        prompt += "[Round {}]\n\né—®ï¼š{}\n\nç­”ï¼š{}\n\n".format(
            i + 1, old_query, response)
    prompt += "[Round {}]\n\né—®ï¼š{}\n\nç­”ï¼š".format(len(history) + 1, query)
    inputs = tokenizer([prompt], return_tensors="pt")
    inputs = inputs.to(device)
    return inputs


tokenizer = AutoTokenizer.from_pretrained(args.model_id,
                                          trust_remote_code=True)
model = AutoModel.from_pretrained(args.model_id,
                                  trust_remote_code=True).float()

device = 'cpu'
model.eval()
# input_tensors
input_tensors = build_inputs(device, tokenizer, query, history)

print(" --- forward first --- ")
outputs = model.forward(**input_tensors)

print("--- second forward ---")
attention_mask = input_tensors["attention_mask"]
position_ids = input_tensors["position_ids"]
past_key_values = outputs["past_key_values"]
# copy from forward in second time
input_ids = torch.tensor([[30910]]).to(device)

# copy from _update_model_kwargs_for_generation in modeling_chatglm.py
attention_mask = torch.cat(
    [attention_mask,
     attention_mask.new_ones((attention_mask.shape[0], 1))],
    dim=-1)
new_position_id = position_ids[..., -1:].clone()
new_position_id += 1
position_ids = torch.cat([position_ids, new_position_id], dim=-1)
# copy from prepare_inputs_for_generation in modeling_chatglm.py
position_ids = position_ids[..., -1:]
# print shape
print("input_ids shape:", input_ids.shape, "; type:", input_ids.dtype)
print("position_ids shape:", position_ids.shape, "; type: ", input_ids.dtype)
print("attention_mask shape:", attention_mask.shape, "; type: ",
      attention_mask.dtype)
print("one past_key_value shape: ", past_key_values[0][0].shape, "; type:",
      past_key_values[0][0].dtype)
print("logits shape: ", outputs["logits"].shape)
outputs2 = model.forward(input_ids=input_ids,
                         attention_mask=attention_mask,
                         position_ids=position_ids,
                         past_key_values=past_key_values)
print("--- export onnx ---")
# ---prepare for onnx export ---
input_names = ["input_ids", 'position_ids', "attention_mask"]
output_names = ["logits"]
dynamic_axes = {
    'input_ids': {
        0: "batch_size",
        1: "sequence"
    },
    'position_ids': {
        0: "batch_size",
        1: "sequence"
    },
    "attention_mask": {
        0: "batch_size",
        1: "past_sequence + sequence"
    },
    "logits": {
        0: "batch_size",
        1: "sequence"
    }
}
for layer_idx in range(model.config.num_layers):
    # --- input key and value ---
    past_key_name = f"past_key_values.{layer_idx}.key"
    past_value_name = f"past_key_values.{layer_idx}.value"
    input_names += [past_key_name, past_value_name]
    # --- output key and value ---
    present_key_name = f"present_key_values.{layer_idx}.key"
    present_value_name = f"present_key_values.{layer_idx}.value"
    output_names += [present_key_name, present_value_name]
    dynamic_axes.update({
        past_key_name: {
            0: "past_sequence",
            1: "batch_size",
        },
        past_value_name: {
            0: "past_sequence",
            1: "batch_size",
        },
        present_key_name: {
            0: "past_sequence + 1",
            1: "batch_size"
        },
        present_value_name: {
            0: "past_sequence + 1",
            1: "batch_size"
        }
    })

if args.compress_weight == True:
    print("--- compress weight ---")
    from nncf import compress_weights
    model = compress_weights(model)

with torch.no_grad():
    torch.onnx.export(
        model,
        args=(input_ids, position_ids, attention_mask, past_key_values),
        f="onnx_model/chatglm2.onnx",
        opset_version=15,
        input_names=input_names,
        output_names=output_names,
        dynamic_axes=dynamic_axes,
        do_constant_folding=False,
    )

if args.compress_weight == True:
    ov_model = ov.convert_model(onnx_model)
else:
    ov_model = ov.convert_model(onnx_model, compress_to_fp16=True)
serialize(ov_model, str(ir_model))